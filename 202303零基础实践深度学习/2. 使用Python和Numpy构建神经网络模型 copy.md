# ä½¿ç”¨Pythonå’ŒNumpyæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹


æ„å»ºç¥ç»ç½‘ç»œ/æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºæœ¬æ­¥éª¤ï¼š

- æ•°æ®å¤„ç†ã€‚è¯»å–æ•°æ®å¹¶å®Œæˆé¢„å¤„ç†æ“ä½œ
- æ¨¡å‹è®¾è®¡ã€‚ç½‘ç»œç»“æ„è®¾è®¡ï¼Œæ„å»ºæ¨¡å‹çš„å‡è®¾ç©ºé—´ã€‚æ¨¡å‹èƒ½å¤Ÿè¡¨è¾¾çš„å…³ç³»é›†åˆ
- è®­ç»ƒé…ç½®ã€‚è®¾å®šæ¨¡å‹é‡‡ç”¨çš„å¯»è§£ç®—æ³•ï¼Œå¹¶æŒ‡å®šè®¡ç®—èµ„æºã€‚
- è®­ç»ƒè¿‡ç¨‹ã€‚å¾ªç¯è°ƒç”¨è®­ç»ƒè¿‡ç¨‹ï¼Œæ¯è½®éƒ½åŒ…æ‹¬å‰å‘è®¡ç®—ã€æŸå¤±å‡½æ•°å’Œåå‘ä¼ æ’­ä¸‰ä¸ªæ­¥éª¤ã€‚
- æ¨¡å‹ä¿å­˜ã€‚å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜ï¼Œæ¨¡å‹é¢„æµ‹æ—¶è°ƒç”¨ã€‚
  
## å¤„ç†æ•°æ®

æ•°æ®å¤„ç†åŒ…å«äº”ä¸ªéƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†åï¼Œæ‰èƒ½è¢«æ¨¡å‹è°ƒç”¨ã€‚
- æ•°æ®å¯¼å…¥ã€‚
- æ•°æ®å½¢çŠ¶å˜æ¢ã€‚
- æ•°æ®é›†åˆ’åˆ†ã€‚å°†æ•°æ®é›†åˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†ç”¨äºç¡®å®šæ¨¡å‹çš„å‚æ•°ï¼Œæµ‹è¯•é›†ç”¨äºè¯„åˆ¤æ¨¡å‹çš„æ•ˆæœ
- æ•°æ®å½’ä¸€åŒ–å¤„ç†ã€‚å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„å–å€¼ç¼©æ”¾åˆ°0~1ä¹‹é—´ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š
  - ä¸€æ˜¯æ¨¡å‹è®­ç»ƒæ›´é«˜æ•ˆï¼›
  - äºŒæ˜¯ç‰¹å¾å‰çš„æƒé‡å¤§å°å¯ä»¥ä»£è¡¨è¯¥å˜é‡å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®åº¦ï¼ˆå› ä¸ºæ¯ä¸ªç‰¹å¾å€¼æœ¬èº«çš„èŒƒå›´ç›¸åŒï¼‰ã€‚
- å°è£…load dataå‡½æ•°ã€‚
  
```python
def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ')

    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 14]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # è®¡ç®—è®­ç»ƒé›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼
    maximums, minimums = training_data.max(axis=0), \
                            training_data.min(axis=0)

    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data
```


## æ¨¡å‹è®¾è®¡

æ¨¡å‹è®¾è®¡æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹å…³é”®è¦ç´ ä¹‹ä¸€ï¼Œä¹Ÿç§°ä¸ºç½‘ç»œç»“æ„è®¾è®¡ï¼Œç›¸å½“äºæ¨¡å‹çš„å‡è®¾ç©ºé—´ï¼Œå³å®ç°æ¨¡å‹â€œå‰å‘è®¡ç®—â€ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰çš„è¿‡ç¨‹ã€‚

ç¥ç»ç½‘ç»œçš„ä¸‰ä¸ªå…³é”®ï¼šåŠ æƒå’Œï¼ˆç•™ä¸‹ï¼‰ï¼Œéçº¿æ€§å˜æ¢ï¼ˆå»æ‰ï¼‰ï¼Œå¤šå±‚è¿æ¥ï¼ˆå»æ‰ï¼‰



ä¸ºä½•å¾ˆå¤šä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç®—æ³•å‡å¯ä»¥ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ›¿æ¢ï¼Ÿ

æ·±åº¦å­¦ä¹ ç½‘ç»œå¯ä»¥è¡¨è¾¾


ç¥ç»ç½‘ç»œå°±æ˜¯ä¸€ç³»åˆ—çš„å‘é‡è¿ç®—ã€‚

```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œ
        # æ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
```

## è®­ç»ƒé…ç½®

æ¨¡å‹è®¾è®¡å®Œæˆåï¼Œéœ€è¦é€šè¿‡è®­ç»ƒé…ç½®å¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å€¼ï¼Œå³é€šè¿‡æŸå¤±å‡½æ•°æ¥è¡¡é‡æ¨¡å‹çš„å¥½åã€‚

åœ¨å›å½’é—®é¢˜ä¸­å¸¸ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè€Œåœ¨åˆ†ç±»é—®é¢˜ä¸­å¸¸ç”¨é‡‡ç”¨äº¤å‰ç†µï¼ˆCross-Entropyï¼‰ä½œä¸ºæŸå¤±å‡½æ•°.

```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        cost = error * error
        cost = np.mean(cost)
        return cost

```

## è®­ç»ƒè¿‡ç¨‹

æ±‚è§£å‚æ•° ğ‘¤ å’Œ ğ‘ çš„æ•°å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿç§°ä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚è®­ç»ƒè¿‡ç¨‹æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å…³é”®è¦ç´ ä¹‹ä¸€ï¼Œå…¶ç›®æ ‡æ˜¯è®©å®šä¹‰çš„æŸå¤±å‡½æ•°å°½å¯èƒ½çš„å°ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰¾åˆ°ä¸€ä¸ªå‚æ•°è§£ ğ‘¤ å’Œ ğ‘ ï¼Œä½¿å¾—æŸå¤±å‡½æ•°å–å¾—æå°å€¼ã€‚

æ•°å€¼æ±‚è§£æ–¹æ³•ï¼š

æ¢¯åº¦ä¸‹é™æ³•ã€‚ä»å½“å‰çš„å‚æ•°å–å€¼ï¼Œä¸€æ­¥æ­¥çš„æŒ‰ç…§ä¸‹å¡çš„æ–¹å‘ä¸‹é™ï¼Œç›´åˆ°èµ°åˆ°æœ€ä½ç‚¹ã€‚

```python
net = Network(13)
losses = []
#åªç”»å‡ºå‚æ•°w5å’Œw9åœ¨åŒºé—´[-160, 160]çš„æ›²çº¿éƒ¨åˆ†ï¼Œä»¥åŠåŒ…å«æŸå¤±å‡½æ•°çš„æå€¼
w5 = np.arange(-160.0, 160.0, 1.0)
w9 = np.arange(-160.0, 160.0, 1.0)
losses = np.zeros([len(w5), len(w9)])

#è®¡ç®—è®¾å®šåŒºåŸŸå†…æ¯ä¸ªå‚æ•°å–å€¼æ‰€å¯¹åº”çš„Loss
for i in range(len(w5)):
    for j in range(len(w9)):
        net.w[5] = w5[i]
        net.w[9] = w9[j]
        z = net.forward(x)
        loss = net.loss(z, y)
        losses[i, j] = loss

#ä½¿ç”¨matplotlibå°†ä¸¤ä¸ªå˜é‡å’Œå¯¹åº”çš„Lossä½œ3Då›¾
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = Axes3D(fig)

w5, w9 = np.meshgrid(w5, w9)

ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')
plt.show()
```


å‡æ–¹è¯¯å·®å’Œç»å¯¹å€¼è¯¯å·®ï¼ˆåªå°†æ¯ä¸ªæ ·æœ¬çš„è¯¯å·®ç´¯åŠ ï¼Œä¸åšå¹³æ–¹å¤„ç†ï¼‰çš„æŸå¤±å‡½æ•°æ›²çº¿å›¾:

![20230304164326](https://cdn.jsdelivr.net/gh/ironartisan/picRepo/20230304164326.png)

å‡æ–¹è¯¯å·®è¡¨ç°çš„â€œåœ†æ»‘â€çš„å¡åº¦æœ‰ä¸¤ä¸ªå¥½å¤„ï¼š
- æ›²çº¿çš„æœ€ä½ç‚¹æ˜¯å¯å¯¼çš„ã€‚
- è¶Šæ¥è¿‘æœ€ä½ç‚¹ï¼Œæ›²çº¿çš„å¡åº¦é€æ¸æ”¾ç¼“ï¼Œæœ‰åŠ©äºé€šè¿‡å½“å‰çš„æ¢¯åº¦æ¥åˆ¤æ–­æ¥è¿‘æœ€ä½ç‚¹çš„ç¨‹åº¦ï¼ˆæ˜¯å¦é€æ¸å‡å°‘æ­¥é•¿ï¼Œä»¥å…é”™è¿‡æœ€ä½ç‚¹ï¼‰ã€‚

è€Œç»å¯¹å€¼è¯¯å·®æ˜¯ä¸å…·å¤‡è¿™ä¸¤ä¸ªç‰¹æ€§çš„ï¼Œè¿™ä¹Ÿæ˜¯æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸ä»…ä»…è¦è€ƒè™‘â€œåˆç†æ€§â€ï¼Œè¿˜è¦è¿½æ±‚â€œæ˜“è§£æ€§â€çš„åŸå› ã€‚

Numpyåº“çš„å¹¿æ’­åŠŸèƒ½ï¼š

ä¸€æ–¹é¢å¯ä»¥æ‰©å±•å‚æ•°çš„ç»´åº¦ï¼Œä»£æ›¿forå¾ªç¯æ¥è®¡ç®—1ä¸ªæ ·æœ¬å¯¹ä» ğ‘¤0 åˆ° ğ‘¤12 çš„æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚
å¦ä¸€æ–¹é¢å¯ä»¥æ‰©å±•æ ·æœ¬çš„ç»´åº¦ï¼Œä»£æ›¿forå¾ªç¯æ¥è®¡ç®—æ ·æœ¬0åˆ°æ ·æœ¬403å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚

```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)
        
        return gradient_w, gradient_b
```
### ç¡®å®šæŸå¤±å‡½æ•°æ›´å°çš„ç‚¹

ä¸ºä»€ä¹ˆä¹‹å‰æˆ‘ä»¬è¦åšè¾“å…¥ç‰¹å¾çš„å½’ä¸€åŒ–ï¼Œä¿æŒå°ºåº¦ä¸€è‡´ï¼Ÿè¿™æ˜¯ä¸ºäº†è®©ç»Ÿä¸€çš„æ­¥é•¿æ›´åŠ åˆé€‚ã€‚ç‰¹å¾è¾“å…¥å½’ä¸€åŒ–åï¼Œä¸åŒå‚æ•°è¾“å‡ºçš„Lossæ˜¯ä¸€ä¸ªæ¯”è¾ƒè§„æ•´çš„æ›²çº¿ï¼Œå­¦ä¹ ç‡å¯ä»¥è®¾ç½®æˆç»Ÿä¸€çš„å€¼ ï¼›ç‰¹å¾è¾“å…¥æœªå½’ä¸€åŒ–æ—¶ï¼Œä¸åŒç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰€éœ€çš„æ­¥é•¿ä¸ä¸€è‡´ï¼Œå°ºåº¦è¾ƒå¤§çš„å‚æ•°éœ€è¦å¤§æ­¥é•¿ï¼Œå°ºå¯¸è¾ƒå°çš„å‚æ•°éœ€è¦å°æ­¥é•¿ï¼Œå¯¼è‡´æ— æ³•è®¾ç½®ç»Ÿä¸€çš„å­¦ä¹ ç‡ã€‚


### è®­ç»ƒæ‰©å±•åˆ°å…¨éƒ¨å‚æ•°

```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)        
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
        
    def train(self, x, y, iterations=100, eta=0.01):
        losses = []
        for i in range(iterations):
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_w, gradient_b = self.gradient(x, y)
            self.update(gradient_w, gradient_b, eta)
            losses.append(L)
            if (i+1) % 10 == 0:
                print('iter {}, loss {}'.format(i, L))
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# åˆ›å»ºç½‘ç»œ
net = Network(13)
num_iterations=1000
# å¯åŠ¨è®­ç»ƒ
losses = net.train(x,y, iterations=num_iterations, eta=0.01)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

## éšæœºæ¢¯åº¦ä¸‹é™

ä½†åœ¨å®é™…é—®é¢˜ä¸­ï¼Œæ•°æ®é›†å¾€å¾€éå¸¸å¤§ï¼Œå¦‚æœæ¯æ¬¡éƒ½ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œæ•ˆç‡éå¸¸ä½ï¼Œé€šä¿—åœ°è¯´å°±æ˜¯â€œæ€é¸¡ç„‰ç”¨ç‰›åˆ€â€ã€‚ç”±äºå‚æ•°æ¯æ¬¡åªæ²¿ç€æ¢¯åº¦åæ–¹å‘æ›´æ–°ä¸€ç‚¹ç‚¹ï¼Œå› æ­¤æ–¹å‘å¹¶ä¸éœ€è¦é‚£ä¹ˆç²¾ç¡®ã€‚ä¸€ä¸ªåˆç†çš„è§£å†³æ–¹æ¡ˆæ˜¯æ¯æ¬¡ä»æ€»çš„æ•°æ®é›†ä¸­éšæœºæŠ½å–å‡ºå°éƒ¨åˆ†æ•°æ®æ¥ä»£è¡¨æ•´ä½“ï¼ŒåŸºäºè¿™éƒ¨åˆ†æ•°æ®è®¡ç®—æ¢¯åº¦å’ŒæŸå¤±æ¥æ›´æ–°å‚æ•°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä½œéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼ŒSGDï¼‰ï¼Œæ ¸å¿ƒæ¦‚å¿µå¦‚ä¸‹ï¼š

- minibatchï¼šæ¯æ¬¡è¿­ä»£æ—¶æŠ½å–å‡ºæ¥çš„ä¸€æ‰¹æ•°æ®è¢«ç§°ä¸ºä¸€ä¸ªminibatchã€‚
- batch sizeï¼šæ¯ä¸ªminibatchæ‰€åŒ…å«çš„æ ·æœ¬æ•°ç›®ç§°ä¸ºbatch sizeã€‚
- Epochï¼šå½“ç¨‹åºè¿­ä»£çš„æ—¶å€™ï¼ŒæŒ‰minibatché€æ¸æŠ½å–å‡ºæ ·æœ¬ï¼Œå½“æŠŠæ•´ä¸ªæ•°æ®é›†éƒ½éå†åˆ°äº†çš„æ—¶å€™ï¼Œåˆ™å®Œæˆäº†ä¸€è½®è®­ç»ƒï¼Œä¹Ÿå«ä¸€ä¸ªEpochï¼ˆè½®æ¬¡ï¼‰ã€‚å¯åŠ¨è®­ç»ƒæ—¶ï¼Œå¯ä»¥å°†è®­ç»ƒçš„è½®æ•°num_epochså’Œbatch_sizeä½œä¸ºå‚æ•°ä¼ å…¥ã€‚


é€šè¿‡å¤§é‡å®éªŒå‘ç°ï¼Œæ¨¡å‹å¯¹æœ€åå‡ºç°çš„æ•°æ®å°è±¡æ›´åŠ æ·±åˆ»ã€‚è®­ç»ƒæ•°æ®å¯¼å…¥åï¼Œè¶Šæ¥è¿‘æ¨¡å‹è®­ç»ƒç»“æŸï¼Œæœ€åå‡ ä¸ªæ‰¹æ¬¡æ•°æ®å¯¹æ¨¡å‹å‚æ•°çš„å½±å“è¶Šå¤§ã€‚ä¸ºäº†é¿å…æ¨¡å‹è®°å¿†å½±å“è®­ç»ƒæ•ˆæœï¼Œéœ€è¦è¿›è¡Œæ ·æœ¬ä¹±åºæ“ä½œã€‚

trainå‡½æ•°çš„ä»£ç å¦‚ä¸‹ï¼š
```python
# è·å–æ•°æ®
train_data, test_data = load_data()

# æ‰“ä¹±æ ·æœ¬é¡ºåº
np.random.shuffle(train_data)

# å°†train_dataåˆ†æˆå¤šä¸ªminibatch
batch_size = 10
n = len(train_data)
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]

# åˆ›å»ºç½‘ç»œ
net = Network(13)

# ä¾æ¬¡ä½¿ç”¨æ¯ä¸ªmini_batchçš„æ•°æ®
for mini_batch in mini_batches:
    x = mini_batch[:, :-1]
    y = mini_batch[:, -1:]
    loss = net.train(x, y, iterations=1)
```
å°†æ¯ä¸ªéšæœºæŠ½å–çš„minibatchæ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­ç”¨äºå‚æ•°è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒæ˜¯ä¸¤å±‚å¾ªç¯ï¼š

ç¬¬ä¸€å±‚å¾ªç¯ï¼Œä»£è¡¨æ ·æœ¬é›†åˆè¦è¢«è®­ç»ƒéå†å‡ æ¬¡ï¼Œç§°ä¸ºâ€œepochâ€ï¼Œä»£ç å¦‚ä¸‹ï¼š
`for epoch_id in range(num_epochs):`

ç¬¬äºŒå±‚å¾ªç¯ï¼Œä»£è¡¨æ¯æ¬¡éå†æ—¶ï¼Œæ ·æœ¬é›†åˆè¢«æ‹†åˆ†æˆçš„å¤šä¸ªæ‰¹æ¬¡ï¼Œéœ€è¦å…¨éƒ¨æ‰§è¡Œè®­ç»ƒï¼Œç§°ä¸ºâ€œiter (iteration)â€ï¼Œä»£ç å¦‚ä¸‹ï¼š

`for iter_id,mini_batch in emumerate(mini_batches):`

åœ¨ä¸¤å±‚å¾ªç¯çš„å†…éƒ¨æ˜¯ç»å…¸çš„å››æ­¥è®­ç»ƒæµç¨‹ï¼šå‰å‘è®¡ç®—->è®¡ç®—æŸå¤±->è®¡ç®—æ¢¯åº¦->æ›´æ–°å‚æ•°ï¼Œè¿™ä¸å¤§å®¶ä¹‹å‰æ‰€å­¦æ˜¯ä¸€è‡´çš„ï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
x = mini_batch[:, :-1]
y = mini_batch[:, -1:]
a = self.forward(x)  #å‰å‘è®¡ç®—
loss = self.loss(a, y)  #è®¡ç®—æŸå¤±
gradient_w, gradient_b = self.gradient(x, y)  #è®¡ç®—æ¢¯åº¦
self.update(gradient_w, gradient_b, eta)  #æ›´æ–°å‚æ•°
```

å°†ä¸¤éƒ¨åˆ†æ”¹å†™çš„ä»£ç é›†æˆåˆ°Networkç±»ä¸­çš„trainå‡½æ•°ä¸­ï¼Œæœ€ç»ˆçš„å®ç°å¦‚ä¸‹ã€‚

```python

import numpy as np

class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        #np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        N = x.shape[0]
        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = 1. / N * np.sum(z-y)
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
            
                
    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):
        n = len(training_data)
        losses = []
        for epoch_id in range(num_epochs):
            # åœ¨æ¯è½®è¿­ä»£å¼€å§‹ä¹‹å‰ï¼Œå°†è®­ç»ƒæ•°æ®çš„é¡ºåºéšæœºæ‰“ä¹±
            # ç„¶åå†æŒ‰æ¯æ¬¡å–batch_sizeæ¡æ•°æ®çš„æ–¹å¼å–å‡º
            np.random.shuffle(training_data)
            # å°†è®­ç»ƒæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œæ¯ä¸ªmini_batchåŒ…å«batch_sizeæ¡çš„æ•°æ®
            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]
            for iter_id, mini_batch in enumerate(mini_batches):
                #print(self.w.shape)
                #print(self.b)
                x = mini_batch[:, :-1]
                y = mini_batch[:, -1:]
                a = self.forward(x)
                loss = self.loss(a, y)
                gradient_w, gradient_b = self.gradient(x, y)
                self.update(gradient_w, gradient_b, eta)
                losses.append(loss)
                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.
                                 format(epoch_id, iter_id, loss))
        
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()

# åˆ›å»ºç½‘ç»œ
net = Network(13)
# å¯åŠ¨è®­ç»ƒ
losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(len(losses))
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

éšæœºæ¢¯åº¦ä¸‹é™åŠ å¿«äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä½†ç”±äºæ¯æ¬¡ä»…åŸºäºå°‘é‡æ ·æœ¬æ›´æ–°å‚æ•°å’Œè®¡ç®—æŸå¤±iiï¼Œæ‰€ä»¥æŸå¤±ä¸‹é™æ›²çº¿ä¼šå‡ºç°éœ‡è¡ã€‚

## æ€»ç»“

æœ¬èŠ‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨Numpyå®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ„å»ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹å®ç°æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ï¼Œå¯ä»¥æ€»ç»“å‡ºï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡æˆ¿ä»·é¢„æµ‹æœ‰ä¸‰ä¸ªè¦ç‚¹ï¼š

æ„å»ºç½‘ç»œï¼Œåˆå§‹åŒ–å‚æ•°wwwå’Œbbbï¼Œå®šä¹‰é¢„æµ‹å’ŒæŸå¤±å‡½æ•°çš„è®¡ç®—æ–¹æ³•ã€‚
éšæœºé€‰æ‹©åˆå§‹ç‚¹ï¼Œå»ºç«‹æ¢¯åº¦çš„è®¡ç®—æ–¹æ³•å’Œå‚æ•°æ›´æ–°æ–¹å¼ã€‚
å°†æ•°æ®é›†çš„æ•°æ®æŒ‰batch sizeçš„å¤§å°åˆ†æˆå¤šä¸ªminibatchï¼Œåˆ†åˆ«çŒå…¥æ¨¡å‹è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ï¼Œä¸æ–­è¿­ä»£ç›´åˆ°æŸå¤±å‡½æ•°å‡ ä¹ä¸å†ä¸‹é™ã€‚



